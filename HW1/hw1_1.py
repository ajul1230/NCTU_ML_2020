# -*- coding: utf-8 -*-
"""HW1_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-UA8fd4VqkXsrR0j2pXbB5w2d_-a044j
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

"""先將data_X及data_T引入並分別標為x跟y。我一開始就將x的資料作標準化並打亂順序，以防輸入資料是照某種規則排列的。總共500筆輸入資料，我取400筆當作Training data，剩下即為Validation set。"""

x = pd.read_csv('data_X.csv').drop('Serial_id',axis=1)
y = pd.read_csv('data_T.csv').drop('Serial_id',axis=1)
x = (x-x.mean())/(x.std())
x = pd.concat([x,y],axis=1)
x = x.reindex(np.random.permutation(x.index))

x_training = x.iloc[:400,:-1]
x_valid = x.iloc[400:,:-1]
y_training = pd.DataFrame(x.iloc[:400,-1])
y_valid = pd.DataFrame(x.iloc[400:,-1])

x_training

"""第一大題我想嘗試使用Gradient Descent的方法來實作，因此我先寫下能計算出最後Weight、以及誤差、RMS的函式。"""

def gradient(x,y,lr,iterations):
    weight = [0]*(len(x.columns)+1) #先將一開始的權重皆設為0
    e = [] #用來記錄Error的list
    j = 0
    while j<iterations:
        t = weight[1:]*x.iloc[:,0:] #算出每個輸入乘上各自權重
        tn = np.sum(t,axis=1)+weight[0] #加上bias項
        er = error(y,t,weight) #計算Error
        for i in range(0,len(weight)): #Gradient Descent
            if i==0:
                weight[0] = weight[0]-lr*(sum((tn-y.iloc[:,0]))/len(x))
            else:
                weight[i] = weight[i]-lr*(sum((tn-y.iloc[:,0])*x.iloc[:,i-1])/len(x))
        e.append(er) 
        j += 1
    return e,weight #返回紀錄每次的Error以及最後的權重

def error(y,t,weight):
    er = np.sum(t,axis=1)+weight[0]
    return np.sum((er-y.iloc[:,0])**2)/(2*len(y))

def rms(data,target,weight):
    data = data*weight[1:]
    temp = error(target,data,weight)
    return (2*temp)**0.5 #在先算Error時已經除以過N了

"""# (a)

M=1，設定learning rate為0.01，迭代次數為2000次，由畫出的Error圖也可看出收斂的速度，迭代2000次應是足夠的，最後也印出Training set及Valid set的RMS。
"""

error_training,weight_training = gradient(x_training,y_training,0.01,2000)

plt.figure()
plt.plot(range(len(error_training)),error_training,'r--',label = 'training')
plt.legend(loc='best')
plt.xlabel('iterations')
plt.ylabel('Error')

print("M=1\nTraining RMS = {}\nValid RMS = {}".format(rms(x_training,y_training,weight_training),rms(x_valid,y_valid,weight_training)))

"""M=2，依照題目給的公式先將input做成原本的feature再加上各自兩兩相乘的型態，一樣再代入learning rate = 0.01、迭代2000次的Gradient descent算出最後的RMS。"""

def TwoDimDf(x):
    m = len(x.columns)
    for i in range(m):
        for j in range(m):
            x = pd.concat([x,x.iloc[:,i]*x.iloc[:,j]],axis=1)
    return x

x_training2 = TwoDimDf(x_training)
x_valid2 = TwoDimDf(x_valid)

x_training2

error_training2,weight_training2 = gradient(x_training2,y_training,0.01,2000)

plt.figure()
plt.plot(range(len(error_training2)),error_training2,'r--',label = 'training')
plt.legend(loc='best')
plt.xlabel('iterations')
plt.ylabel('Error')

print("M=2\nTraining RMS = {}\nValid RMS = {}".format(rms(x_training2,y_training,weight_training2),rms(x_valid2,y_valid,weight_training2)))

"""由結果可看出在M=1時，雖然Training的RMS表現較M=2時差，但在Valid的RMS表現卻是比較好的。這有可能是在M=2時比M=1有更多over-fitting的問題。

## (b)

我使用L1 Regularization的性質來決定最有貢獻的feature。由於L1的性質能讓更多的參數變為0，利用此特性可用來做feature selection。我在誤差函數中加上Regularization項並套用在gradient descent中，來找出重要的參數。
"""

def gradientReg(x,y,lr,Lambda,iterations):
    weight = [0]*(len(x.columns)+1)
    e = []
    j = 0
    while j<iterations:
        t = weight[1:]*x.iloc[:,0:]
        tn = np.sum(t,axis=1)+weight[0]
        er = error(y,t,weight)
        for i in range(0,len(weight)):
            if i==0:
                weight[0] = weight[0]-lr*(sum((tn-y.iloc[:,0]))/len(x)+Lambda/2)
            else:
                weight[i] = weight[i]-lr*(sum((tn-y.iloc[:,0])*x.iloc[:,i-1])/len(x)+Lambda/2)
        e.append(er)
        j += 1
    return e,weight

error_training3,weight_training3 = gradientReg(x_training,y_training,0.01,0.01,2000)

weight_training3

"""由上結果可知，CGPA對照的權重參數是最大的，因此可判斷CGPA為最有貢獻的特徵。而SOP及University則相對貢獻較小。"""

